{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import set_global_service_context\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import TokenTextSplitter\n",
    "import together\n",
    "import logging\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "from pydantic import Extra, Field, root_validator\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import TogetherEmbedding\n",
    "#import chromadb\n",
    "#from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./ML-DS-TEXTBOOKS\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "  separator=\" \",\n",
    "  chunk_size=1024,\n",
    "  chunk_overlap=20,\n",
    "  backup_separators=[\"\\n\"],\n",
    "  tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    "\n",
    ")\n",
    "\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "together.api_key = os.environ[\"TOGETHER_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\j\\AppData\\Local\\Temp\\ipykernel_15640\\630357882.py:17: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
    "    \"\"\"Together API Key\"\"\"\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"what sampling temperature to use. \"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion. \"\"\"\n",
    "\n",
    "    class Config:\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    #root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the API key is set. \"\"\"\n",
    "        api_key = get_from_dict_or_env(\n",
    "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
    "        )\n",
    "        values[\"together_api_key\"] = api_key\n",
    "        return values\n",
    "\n",
    "    #property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Rteurn type of LLM. \"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint. \"\"\"\n",
    "        together.api_key = self.together_api_key\n",
    "        output =  together.Complete.create(prompt,\n",
    "                                           model=self.model,\n",
    "                                           max_tokens=self.max_tokens,\n",
    "                                           temperature=self.temperature,\n",
    "                                           )\n",
    "        text = output['output']['choices'][0]['text']\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "together_llm = TogetherLLM(\n",
    "    model = \"togethercomputer/llama-2-70b-chat\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = together_llm\n",
    "\n",
    "embed_model = \"local\"\n",
    "\n",
    "prompt_helper = PromptHelper(\n",
    "\n",
    "  context_window=4096, \n",
    "\n",
    "  num_output=256, \n",
    "\n",
    "  chunk_overlap_ratio=0.1, \n",
    "\n",
    "  chunk_size_limit=None\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\j\\python\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "\n",
    "  llm=llm,\n",
    "\n",
    "  embed_model=embed_model,\n",
    "\n",
    "  node_parser=text_splitter,\n",
    "\n",
    "  prompt_helper=prompt_helper\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    service_context = service_context\n",
    "    #storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 794/794 [00:00<00:00, 57.9kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 62.5M/62.5M [00:27<00:00, 2.29MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 316/316 [00:00<00:00, 18.5kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 890kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 4.26kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the reranker\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3 # Note here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_engine = index.as_query_engine(similarity_top_k=2, llm=llm)\n",
    "query_engine = index.as_query_engine(similarity_top_k=5, node_postprocessors=[rerank], llm=llm)\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"You are a helpful Machine Learning assistant. Given a query follow the following steps: think through the question carefully and in detail, split the question into sub-questions, solve the first sub-question, move on and solve the next sub-question, When you have solved all sub-questions combine the knowledge and information gotten, and solve the main question. Your response/answer should be conprehensive and not contradicted with the following context if they are relevant, otherwise ignore them if they are not relevant and . If you can not find any good answer \\n\"\n",
    "    \"Query:  {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = PromptTemplate(template)\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"What  is  repeated  cross-validation  and  why  would  you  prefer  it  to  using  a  single validation set?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Repeated cross-validation is a technique used in machine learning to evaluate the performance of a model by training and testing it multiple times on different subsets of the data. In contrast to using a single validation set, repeated cross-validation provides a more robust estimate of a model's performance by averaging the performance over multiple iterations.\n",
      "\n",
      "The main advantage of repeated cross-validation is that it reduces the variance in the performance estimate compared to using a single validation set. With a single validation set, the performance of the model can be heavily influenced by the specific samples in the set, leading to a biased estimate of the model's true performance. Repeated cross-validation helps to mitigate this issue by averaging the performance over multiple iterations, providing a more reliable estimate of the model's performance.\n",
      "\n",
      "Another advantage of repeated cross-validation is that it allows for a more thorough exploration of the hyperparameter space. By training and testing the model multiple times on different subsets of the data, repeated cross-validation can help identify the hyperparameters that result in the best performance across multiple iterations.\n",
      "\n",
      "In summary, repeated cross-validation is a valuable technique for evaluating the performance of a machine learning model. It provides a more robust estimate of the model's performance by averaging the performance over multiple iterations, and allows for a more thorough exploration of the hyperparameter space.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
