{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import set_global_service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./ML-DS-TEXTBOOKS\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "  separator=\" \",\n",
    "  chunk_size=1024,\n",
    "  chunk_overlap=20,\n",
    "  backup_separators=[\"\\n\"],\n",
    "  tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    "\n",
    ")\n",
    "\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import together\n",
    "\n",
    "together.api_key = os.environ[\"TOGETHER_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "from pydantic import Extra, Field, root_validator\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
    "    \"\"\"Together API Key\"\"\"\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"what sampling temperature to use. \"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion. \"\"\"\n",
    "\n",
    "    class Config:\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    #root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the API key is set. \"\"\"\n",
    "        api_key = get_from_dict_or_env(\n",
    "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
    "        )\n",
    "        values[\"together_api_key\"] = api_key\n",
    "        return values\n",
    "\n",
    "    #property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Rteurn type of LLM. \"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint. \"\"\"\n",
    "        together.api_key = self.together_api_key\n",
    "        output =  together.Complete.create(prompt,\n",
    "                                           model=self.model,\n",
    "                                           max_tokens=self.max_tokens,\n",
    "                                           temperature=self.temperature,\n",
    "                                           )\n",
    "        text = output['output']['choices'][0]['text']\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llms = TogetherLLM(\n",
    "    model = \"togethercomputer/llama-2-70b-chat\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import TogetherEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = TogetherEmbedding(\n",
    "    model_name=\"togethercomputer/m2-bert-80M-8k-retrieval\", api_key=together.api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\j\\python\\lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "instructor_embedding = HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-xl',\n",
    "                                                     model_kwargs={\"device\": \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = test_llms\n",
    "\n",
    "embed_model = \"local\"\n",
    "\n",
    "prompt_helper = PromptHelper(\n",
    "\n",
    "  context_window=4096, \n",
    "\n",
    "  num_output=256, \n",
    "\n",
    "  chunk_overlap_ratio=0.1, \n",
    "\n",
    "  chunk_size_limit=None\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 684/684 [00:00<00:00, 34.7kB/s]\n",
      "model.safetensors: 100%|██████████| 133M/133M [00:37<00:00, 3.55MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<00:00, 85.5kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 543kB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 1.08MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 31.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "\n",
    "  llm=llm,\n",
    "\n",
    "  embed_model=embed_model,\n",
    "\n",
    "  node_parser=text_splitter,\n",
    "\n",
    "  prompt_helper=prompt_helper\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection = db.get_or_create_collection(\"forML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets. They are simple to understand and interpret, easy to use, versatile, and powerful. However, they do have a few limitations. First, they are sensitive to training set rotation. Second, they are very sensitive to small variations in the training data.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are decision trees?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    service_context = service_context\n",
    "    #storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\n",
    "\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
